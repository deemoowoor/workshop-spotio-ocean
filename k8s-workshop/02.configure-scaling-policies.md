# Configure Scaling Policies

In this section we'll use HPA (Horizontal Pod Autoscaler) in order to create scaling policies for our applications. HPA requires installation of metric-server in our cluster. The metric-server collects utilization data of all the running pods in the system. Installation of metrics-server can be done with the instruction in the GitHub repo of the project, or by helm.

1. GitHub - https://github.com/kubernetes-incubator/metrics-server
2. Helm - `helm install stable/metrics-server \ --name metrics-server \ --version 2.0.4 \ --namespace metrics`

---

NOTE: If you choose to install metric-server with helm - first make sure that helm is installed - use the instructions from the helm [repository](https://github.com/helm/helm/blob/master/docs/install.md#from-homebrew-macos). After that make sure to [install](https://github.com/helm/helm/blob/master/docs/install.md#easy-in-cluster-installation) tiller on your cluster

---

In order to verify that our metric server is running, use the following command: `kubectl get apiservice v1beta1.metrics.k8s.io -o yaml`

You can also try and `top` any object like `pod` or `node` - `kubectl top pod` will display utilization information about the pods in the current namespace

After installaing our metric server, we can create scaling policies - HPAs for our applications:

---

## Frontend HPA

Run the following commands in order to start the Frontend HPA:

```
kubectl apply -f ./sample-app/frontend/frontend-hpa.yaml
```

---

## Crystal Backend HPA

Run the following commands in order to start the Crystal backend HPA:

```
kubectl apply -f ./sample-app/crystal/crystal-hpa.yaml
```

---

## NodeJS Backend HPA

Run the following commands in order to start the NodeJS backend HPA:

```
kubectl apply -f ./sample-app/nodejs/nodejs-hpa.yaml
```

---

After applying all the HPAs, run `kubectl get hpa` and see the actual utilization of the pods vs the defined scaling policy in the HPA   

## Let's see that in action

Our frontend app is built in a way that creates load on the different layers, it constantly refreshes the browser which causes the load to increase.

That will cause scaling activites as configured in our HPA.

Our scaling policies are configured to scale on a very low CPU precentage on purpose, wait a few moments and see the different deployments scale up.

Run `kubectl get pods` to see the running pods.

After the pods had been scaled up, go back to the browser and see that the different pointers are constantly changing based on the location of the active pod.